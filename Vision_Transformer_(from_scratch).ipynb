{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUfxY2KvGcZwrtFjmCEKwM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junjiezhu98/Vision-Transformer/blob/main/Vision_Transformer_(from_scratch).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbdMnWkB70eN",
        "outputId": "f0e13b56-00a6-460c-bbbf-d1d15d8c2342"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m329.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange"
      ],
      "metadata": {
        "id": "rRbKOPL_8CUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, embed_size=768, patch_size=16, channels=3, img_size=224):\n",
        "        super(PatchEmbedding, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        # Version 1.0\n",
        "        # self.patch_projection = nn.Sequential(\n",
        "        #     Rearrange(\"b c (h h1) (w w1) -> b (h w) (h1 w1 c)\", h1=patch_size, w1=patch_size),\n",
        "        #     nn.Linear(patch_size * patch_size * channels, embed_size)\n",
        "        # )\n",
        "\n",
        "        # Version 2.0\n",
        "        self.patch_projection = nn.Sequential(\n",
        "            nn.Conv2d(channels, embed_size, kernel_size=(patch_size, patch_size), stride=(patch_size, patch_size)),\n",
        "            Rearrange(\"b e (h) (w) -> b (h w) e\"),\n",
        "        )\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_size))\n",
        "        self.positions = nn.Parameter(torch.randn((img_size // patch_size) ** 2 + 1, embed_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        x = self.patch_projection(x)\n",
        "        # prepend the cls token to the input\n",
        "        cls_tokens = repeat(self.cls_token, \"() n e -> b n e\", b=batch_size)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "        # add position embedding\n",
        "        x += self.positions\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "BVzej7xF8IXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super(Residual, self).__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) + x\n",
        "\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super(PreNorm, self).__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n"
      ],
      "metadata": {
        "id": "Tc5lYehW8Ibo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim=768, n_heads=8, dropout=0.):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embed_dim: dimension of embeding vector output\n",
        "            n_heads: number of self attention heads\n",
        "        \"\"\"\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        self.embed_dim = embed_dim  # 768 dim\n",
        "        self.n_heads = n_heads  # 8\n",
        "        self.head_dim = self.embed_dim // self.n_heads  # 768/8 = 96. each key,query,value will be of 96d\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        self.attn_drop = nn.Dropout(dropout)\n",
        "        # key,query and value matrixes\n",
        "        self.to_qkv = nn.Linear(self.embed_dim, self.embed_dim * 3, bias=False)\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(self.embed_dim, self.embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "           x : a unified vector of key query value\n",
        "        Returns:\n",
        "           output vector from multihead attention\n",
        "        \"\"\"\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        q, k, v = map(lambda t: rearrange(t, \"b n (h d) -> b h n d\", h=self.n_heads), qkv)\n",
        "\n",
        "        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
        "        attn = dots.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        out = torch.einsum('bhij,bhjd->bhid', attn, v)\n",
        "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
        "\n",
        "        out = self.to_out(out)\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "id": "0U9OsXG98IfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim=768, depth=12, n_heads=8, mlp_expansions=4, dropout=0.):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                Residual(PreNorm(dim, MultiHeadAttention(dim, n_heads, dropout))),\n",
        "                Residual(FeedForward(dim, dim * mlp_expansions, dropout))\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x)\n",
        "            x = ff(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "QyptfWJJ8Ih8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, dim=768,\n",
        "                 patch_size=16,\n",
        "                 channels=3,\n",
        "                 img_size=224,\n",
        "                 depth=12,\n",
        "                 n_heads=8,\n",
        "                 mlp_expansions=4,\n",
        "                 dropout=0.,\n",
        "                 num_classes=0,\n",
        "                 global_pool='avg'):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        assert global_pool in ('avg', 'token')\n",
        "        self.global_pool = global_pool\n",
        "        self.patch_embedding = PatchEmbedding(dim, patch_size, channels, img_size)\n",
        "        self.transformer = Transformer(dim, depth, n_heads, mlp_expansions, dropout)\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, num_classes)\n",
        "        ) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.patch_embedding(img)\n",
        "        x = self.transformer(x)\n",
        "        x = x[:, 1:].mean(dim=1) if self.global_pool == 'avg' else x[:, 0]\n",
        "        x = self.mlp_head(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "AN7OQ9HC8Ik2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    images = torch.randn((16, 3, 224, 224)).to(device)\n",
        "    vit = VisionTransformer(num_classes=4, global_pool=\"token\").to(device)\n",
        "    output = vit(images)\n",
        "    print(output)\n",
        "    torch.save(vit.state_dict(), \"model.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94t1OTl68RLR",
        "outputId": "353ca39c-b2b7-46a7-cc27-8bf8e9f3909e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.0939,  0.2378, -1.1270, -0.0281],\n",
            "        [ 0.1003,  0.2271, -1.1060, -0.0237],\n",
            "        [ 0.0803,  0.2296, -1.1216, -0.0377],\n",
            "        [ 0.0633,  0.2566, -1.1128, -0.0415],\n",
            "        [ 0.1013,  0.2429, -1.1111, -0.0321],\n",
            "        [ 0.0954,  0.2095, -1.1367, -0.0387],\n",
            "        [ 0.1272,  0.2365, -1.1429, -0.0118],\n",
            "        [ 0.1015,  0.2657, -1.1057, -0.0148],\n",
            "        [ 0.1220,  0.2479, -1.1224, -0.0466],\n",
            "        [ 0.1177,  0.2751, -1.1207, -0.0342],\n",
            "        [ 0.1203,  0.2603, -1.1283, -0.0344],\n",
            "        [ 0.1105,  0.2568, -1.1419, -0.0299],\n",
            "        [ 0.1052,  0.2420, -1.1279, -0.0357],\n",
            "        [ 0.0908,  0.2457, -1.1591, -0.0577],\n",
            "        [ 0.1034,  0.2489, -1.1494, -0.0493],\n",
            "        [ 0.1104,  0.2250, -1.1105, -0.0152]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DCerjo6c8RN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OZcRX7458RRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-qjVS16u8RTz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}